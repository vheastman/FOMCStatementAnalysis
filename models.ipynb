{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Modelling: LDA and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import lda\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.feature_extraction.text as text\n",
    "import sklearn.naive_bayes as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1994-02-04</td>\n",
       "      <td>Chairman Alan Greenspan announced today that t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1994-03-22</td>\n",
       "      <td>Chairman Alan Greenspan announced today that t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1994-04-18</td>\n",
       "      <td>Chairman Alan Greenspan announced today that t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1994-05-17</td>\n",
       "      <td>The Federal Reserve today announced two action...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1994-08-16</td>\n",
       "      <td>The Federal Reserve announced today the follow...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        date                                               text  \\\n",
       "0           0  1994-02-04  Chairman Alan Greenspan announced today that t...   \n",
       "1           1  1994-03-22  Chairman Alan Greenspan announced today that t...   \n",
       "2           2  1994-04-18  Chairman Alan Greenspan announced today that t...   \n",
       "3           3  1994-05-17  The Federal Reserve today announced two action...   \n",
       "4           4  1994-08-16  The Federal Reserve announced today the follow...   \n",
       "\n",
       "   labels  \n",
       "0       1  \n",
       "1       0  \n",
       "2       1  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the statements\n",
    "statements = pd.read_csv(\"statements_with_labels.csv\")\n",
    "statements.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chairman', 'Alan', 'Greenspan', 'announced', 'today']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the statments\n",
    "statements_tokenized = [word_tokenize(i) for i in statements['text']]\n",
    "print(statements_tokenized[0][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Functions to run on feature sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. (a) Clustering Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "def run_LDA(X, vocab, n_topics=8, n_iter=10):\n",
    "    \"\"\" Function to run LDA clustering on featurized input \n",
    "    \n",
    "    Inputs: \n",
    "        - featurized matrix\n",
    "        - vocabulary to match those features\n",
    "        - n_topics: number of clusters\n",
    "        - n_iter: number of iterations\n",
    "        \n",
    "    \"\"\"\n",
    "    model = lda.LDA(n_topics=n_topics, n_iter=n_iter, random_state=1)\n",
    "    model.fit(X)  \n",
    "    topic_word = model.topic_word_ \n",
    "    n_top_words = 8\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "        print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. (b) Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "def run_SVM(X_train, y_train, X_test, y_test):\n",
    "    \"\"\" Function to run SVM on feature set to predict sentiment of statement \n",
    "    \n",
    "    Inputs:\n",
    "        - featurized matrix\n",
    "        - y: labels\n",
    "        \n",
    "    Returns: \n",
    "        - score of model\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    \n",
    "    # Return score\n",
    "    return clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB\n",
    "def run_NB(X_train, y_train, X_test, y_test):\n",
    "    \"\"\" Function to run SVM on feature set to predict sentiment of statement \n",
    "    \n",
    "    Inputs:\n",
    "        - featurized matrix\n",
    "        - y: labels\n",
    "        \n",
    "    Returns: \n",
    "        - score of model\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    bnb = ms.GridSearchCV(\n",
    "            nb.BernoulliNB(),\n",
    "            param_grid={'alpha': np.logspace(-2., 2., 50)})\n",
    "    bnb.fit(X_train, y_train)\n",
    "    # Return score\n",
    "    return bnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create feature sets and run models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. (a) Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary from statements\n",
    "words = []\n",
    "for i in statements_tokenized:\n",
    "    for j in i:\n",
    "        words.append(j.lower())\n",
    "vocab_base = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1879"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1092"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine longest statements\n",
    "max_length = max([len(i) for i in statements_tokenized])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens by finding vocab indices\n",
    "embeddings = []\n",
    "for d in statements_tokenized:\n",
    "    stmt = []\n",
    "    for h in d:\n",
    "        stmt.append(vocab_base.index(h.lower()))\n",
    "    embeddings.append(stmt)\n",
    "    if len(d) < max_length:\n",
    "        # pad the shorter statements with 0s to make them the same length\n",
    "        padding = max_length - len(d)\n",
    "        stmt.extend(list(np.zeros(padding,dtype=np.int8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 1092)\n"
     ]
    }
   ],
   "source": [
    "# Turn embeddings into np array\n",
    "X_base = np.array([np.array(xi) for xi in embeddings])\n",
    "print(X_base.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 171\n",
      "INFO:lda:vocab_size: 1092\n",
      "INFO:lda:n_words: 64870773\n",
      "INFO:lda:n_topics: 8\n",
      "INFO:lda:n_iter: 10\n",
      "INFO:lda:<0> log likelihood: -569755844\n",
      "INFO:lda:<9> log likelihood: -514249488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: persistently 5-1/2 owing pressure suggest sharply markets reserve\n",
      "Topic 1: growthmore persistently remained 28-day 5-1/2 websites pressure changed\n",
      "Topic 2: growthmore persistently remained duke created Â¼ determining pressure\n",
      "Topic 3: shown show vote storm-related attributable action restrictive going\n",
      "Topic 4: persistently owing measured.nonetheless be balance harvey quite gradually\n",
      "Topic 5: growthmore 2014 solid remained jr extension respect tenders\n",
      "Topic 6: persistently growthmore remained duke pressure 5-1/2 necessary subdued\n",
      "Topic 7: foreign owing weigh measured.nonetheless persistently olson balance executed\n"
     ]
    }
   ],
   "source": [
    "# Run LDA\n",
    "run_LDA(X=X_base, vocab=vocab_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = pd.DataFrame(topic_words)\n",
    "#t.to_csv(\"topic_words1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: SVM on Basic Vectorized Matrix\n",
      "Accuracy: 0.628571428571\n"
     ]
    }
   ],
   "source": [
    "# Run SVM\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = ms.train_test_split(X_base, statements['labels'], test_size=0.2)\n",
    "\n",
    "acc = run_SVM(X_train, y_train, X_test, y_test)\n",
    "print(\"Model 1: SVM on Basic Vectorized Matrix\")\n",
    "print(\"Accuracy: \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy: 0.571428571429\n"
     ]
    }
   ],
   "source": [
    "acc2 = run_NB(X_train, y_train, X_test, y_test)\n",
    "print(\"NB Accuracy: \" + str(acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test string with embedded punctuation and numbers\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    " \n",
    "def clearup(s, chars):\n",
    "    return re.sub('[%s]' % chars, '', s).lower()\n",
    " \n",
    "s = 'This is %a t1e22st !st4ring6 w.it6h 87embed766ded punct,:ua-tion and nu=mbe]rS6.'\n",
    " \n",
    "print(clearup(s, string.punctuation+string.digits))\n",
    "\n",
    "statements_nonums = []\n",
    "for j in statements_tokenized:\n",
    "    state = []\n",
    "    for i in j:\n",
    "        i = clearup(i, string.punctuation+string.digits)\n",
    "        if i:\n",
    "            state.append(i)\n",
    "    statements_nonums.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1771"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new vocabulary\n",
    "words = []\n",
    "for i in statements_nonums:\n",
    "    for j in i:\n",
    "        words.append(j.lower())\n",
    "vocab_nonums = list(set(words))\n",
    "\n",
    "len(vocab_nonums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 words removed from vocab\n"
     ]
    }
   ],
   "source": [
    "# How many words were lost?\n",
    "print(str(len(vocab_base) - len(vocab_nonums)) + ' words removed from vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "967"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine longest statements\n",
    "max_length = max([len(i) for i in statements_nonums])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 967)\n"
     ]
    }
   ],
   "source": [
    "# Get tokens by finding vocab indices\n",
    "embeddings = []\n",
    "for d in statements_nonums:\n",
    "    stmt = []\n",
    "    for h in d:\n",
    "        stmt.append(vocab_nonums.index(h.lower()))\n",
    "    embeddings.append(stmt)\n",
    "    if len(d) < max_length:\n",
    "        padding = max_length - len(d)\n",
    "        stmt.extend(list(np.zeros(padding,dtype=np.int8)))\n",
    "        \n",
    "# Turn embeddings into np array\n",
    "X_nonums = np.array([np.array(xi) for xi in embeddings])\n",
    "print(X_nonums.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 171\n",
      "INFO:lda:vocab_size: 967\n",
      "INFO:lda:n_words: 65237604\n",
      "INFO:lda:n_topics: 8\n",
      "INFO:lda:n_iter: 10\n",
      "INFO:lda:<0> log likelihood: -566405708\n",
      "INFO:lda:<9> log likelihood: -512556364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: requests next targeted cleveland began extended believes strengthened\n",
      "Topic 1: promising directive eased seeks fundamentals banks fostering foreign\n",
      "Topic 2: than having range strained closely that most eroded\n",
      "Topic 3: accompanying willing discussed directors confirms pay facilitate primary\n",
      "Topic 4: next eased requests promising directive medium effect strengthened\n",
      "Topic 5: directive pushed disparity goods stress overtime purchases mainly\n",
      "Topic 6: promising fall region firming better meanwhile eased next\n",
      "Topic 7: next requests targeted implied extended lacker rose exacerbated\n"
     ]
    }
   ],
   "source": [
    "# Run LDA\n",
    "run_LDA(X=X_nonums, vocab=vocab_nonums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: SVM on matrix with numbers removed\n",
      "Accuracy: 0.542857142857\n"
     ]
    }
   ],
   "source": [
    "# Run SVM\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_nonums, statements['labels'], test_size=0.2)\n",
    "\n",
    "acc = run_SVM(X_train, y_train, X_test, y_test)\n",
    "print(\"Model 1: SVM on matrix with numbers removed\")\n",
    "print(\"Accuracy: \" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. tfidVectorizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 1764)\n"
     ]
    }
   ],
   "source": [
    "y = statements['labels']\n",
    "\n",
    "# Implement vectorizer\n",
    "tf = text.TfidfVectorizer()\n",
    "X = tf.fit_transform(statements['text'])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each sample has ~11.18% non-zero features.\n"
     ]
    }
   ],
   "source": [
    "p = 100 * X.nnz / float(X.shape[0] * X.shape[1])\n",
    "print(f\"Each sample has ~{p:.2f}% non-zero features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = ms.train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': array([  1.00000e-02,   1.20679e-02,   1.45635e-02,   1.75751e-02,\n",
       "         2.12095e-02,   2.55955e-02,   3.08884e-02,   3.72759e-02,\n",
       "         4.49843e-02,   5.42868e-02,   6.55129e-02,   7.90604e-02,\n",
       "         9.54095e-02,   1.15140e-01,   1.38950e-01,   1.67683e-01,\n",
       "         2....    3.90694e+01,   4.71487e+01,   5.68987e+01,   6.86649e+01,\n",
       "         8.28643e+01,   1.00000e+02])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb = ms.GridSearchCV(\n",
    "    nb.BernoulliNB(),\n",
    "    param_grid={'alpha': np.logspace(-2., 2., 50)})\n",
    "bnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.571428571429\n"
     ]
    }
   ],
   "source": [
    "print(bnb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 673)\n"
     ]
    }
   ],
   "source": [
    "# Build bigram\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(2,2), min_df = 0)\n",
    "X = cv.fit_transform(statements['text']).toarray()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 1144)\n"
     ]
    }
   ],
   "source": [
    "tf_vec = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vec.fit_transform(statements['text'])\n",
    "print(tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: inflation action policy growth chairman basis approved price needed stability\n",
      "Topic #1: central bank arrangements national reserve actions markets funding banks strains\n",
      "Topic #2: inflation policy labor longer securities term conditions agency funds monetary\n",
      "Topic #3: stability action board contained term decided evans losses points billion\n",
      "Topic #4: inflation growth implied pressures chairman resource sustain policy donald warsh\n",
      "Topic #5: inflation securities reserve policy funds agency conditions operations monetary range\n",
      "Topic #6: action reserve approved growth board discount basis today banks 25\n",
      "Topic #7: markets growth chairman jr credit policy donald monetary kohn period\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=8, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda_model.fit(tf)\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vec.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-and-machine-learning-on-documents-1.html\n",
    "https://gate.ac.uk/sale/nle-svm/svm-ie.pdf\n",
    "https://www.quora.com/How-do-I-train-a-SVM-classifier-from-text-examples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
