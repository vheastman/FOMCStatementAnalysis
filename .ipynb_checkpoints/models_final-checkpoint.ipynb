{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Here are the final models and results. Many other things were tried and most I saved in the `models_failed.py`, but some were even deleted from there. The data was processed in `process_data.py` and details of the statements were explored in `EDA.py`. \n",
    "\n",
    "The purpose of this analysis was to determine word clustering for the 141 statements issued by the FOMC. Relatedly, I attempt to train a model to predict statement 'sentiment'. I measure sentiment by the daily change in the 10-year treasury yield after the statement is released. \n",
    "\n",
    "For the clustering analysis, I found the straight CountVectorizer was the best way to extract features. For the sentiment analysis, I used TfidfVectorizer, which is the same as CountVectorizer followed by the TfidfTransformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import sklearn libraries\n",
    "from sklearn import svm\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.feature_extraction.text as text\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0        date                                               text  \\\n",
      "0           0  1994-02-04  Chairman Alan Greenspan announced today that t...   \n",
      "1           1  1994-03-22  Chairman Alan Greenspan announced today that t...   \n",
      "2           2  1994-04-18  Chairman Alan Greenspan announced today that t...   \n",
      "3           3  1994-05-17  The Federal Reserve today announced two action...   \n",
      "4           4  1994-08-16  The Federal Reserve announced today the follow...   \n",
      "\n",
      "   labels  \n",
      "0       1  \n",
      "1       0  \n",
      "2       1  \n",
      "3       0  \n",
      "4       0  \n",
      "1    91\n",
      "0    80\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read in the statements\n",
    "statements = pd.read_csv(\"statements_with_labels.csv\")\n",
    "print(statements.head())\n",
    "print(statements.labels.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 1592)\n"
     ]
    }
   ],
   "source": [
    "# Extract features with CountVectorizer\n",
    "# Ignore words with over 95% frequency and in less than 2 documents\n",
    "vec = CountVectorizer(max_df=0.90, min_df=1, stop_words='english')\n",
    "\n",
    "vectorized = vec.fit_transform(statements['text'])\n",
    "print(vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: markets states policy emergence financial forces forward inflationary levels conditions\n",
      "Topic #1: inflation policy securities labor conditions longer agency term monetary mortgage\n",
      "Topic #2: bank growth inflation central markets demand banks conditions information arrangements\n",
      "Topic #3: growth approved discount basis today sustainable jr chairman 25 board\n",
      "Topic #4: growth inflation resource markets chairman housing utilization voting kevin warsh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=5, max_iter=5,\n",
    "                                      learning_method='online',\n",
    "                                      learning_offset=50.,\n",
    "                                      random_state=0)\n",
    "lda_model.fit(vectorized)\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "feature_names = vec.get_feature_names()\n",
    "print_top_words(lda_model, feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Sentiment Analysis\n",
    "\n",
    "#### II (a). Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 1764)\n"
     ]
    }
   ],
   "source": [
    "# Identify labels\n",
    "y = statements['labels']\n",
    "\n",
    "# Implement Tfid vectorizer\n",
    "tf = text.TfidfVectorizer()\n",
    "X = tf.fit_transform(statements['text'])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each sample has ~11.18% non-zero features.\n"
     ]
    }
   ],
   "source": [
    "# Identify how many samples have non-zero features\n",
    "p = 100 * X.nnz / float(X.shape[0] * X.shape[1])\n",
    "print(f\"Each sample has ~{p:.2f}% non-zero features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate train/test data\n",
    "(X_train, X_test, y_train, y_test) = ms.train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': array([  1.00000e-02,   1.20679e-02,   1.45635e-02,   1.75751e-02,\n",
       "         2.12095e-02,   2.55955e-02,   3.08884e-02,   3.72759e-02,\n",
       "         4.49843e-02,   5.42868e-02,   6.55129e-02,   7.90604e-02,\n",
       "         9.54095e-02,   1.15140e-01,   1.38950e-01,   1.67683e-01,\n",
       "         2....    3.90694e+01,   4.71487e+01,   5.68987e+01,   6.86649e+01,\n",
       "         8.28643e+01,   1.00000e+02])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GridSearchCV to find optimal alpha\n",
    "nb_model = ms.GridSearchCV(nb.BernoulliNB(), param_grid={'alpha': np.logspace(-2., 2., 50)})\n",
    "nb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65714285714285714"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II (b). SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GridSearchCV \n",
    "svm_model = ms.GridSearchCV(svm.SVC(kernel='rbf'), \n",
    "                            {'C': [0.001, 0.01, 0.1, 1, 10], 'gamma' : [0.001, 0.01, 0.1, 1]}, \n",
    "                            cv=5)\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59999999999999998"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
