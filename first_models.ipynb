{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Modelling: LDA and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import lda\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1994-02-04</td>\n",
       "      <td>Chairman Alan Greenspan announced today that t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1994-03-22</td>\n",
       "      <td>Chairman Alan Greenspan announced today that t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1994-04-18</td>\n",
       "      <td>Chairman Alan Greenspan announced today that t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1994-05-17</td>\n",
       "      <td>The Federal Reserve today announced two action...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1994-08-16</td>\n",
       "      <td>The Federal Reserve announced today the follow...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        date                                               text  \\\n",
       "0           0  1994-02-04  Chairman Alan Greenspan announced today that t...   \n",
       "1           1  1994-03-22  Chairman Alan Greenspan announced today that t...   \n",
       "2           2  1994-04-18  Chairman Alan Greenspan announced today that t...   \n",
       "3           3  1994-05-17  The Federal Reserve today announced two action...   \n",
       "4           4  1994-08-16  The Federal Reserve announced today the follow...   \n",
       "\n",
       "   labels  \n",
       "0       1  \n",
       "1       0  \n",
       "2       1  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the statements\n",
    "statements = pd.read_csv(\"statements_with_labels.csv\")\n",
    "statements.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the statments\n",
    "statements_tokenized = [word_tokenize(i) for i in statements['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary from statements\n",
    "words = []\n",
    "for i in statements_tokenized:\n",
    "    for j in i:\n",
    "        words.append(j.lower())\n",
    "vocab_base = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1879"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1092"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine longest statements\n",
    "max_length = max([len(i) for i in statements_tokenized])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens by finding vocab indices\n",
    "embeddings = []\n",
    "for d in statements_tokenized:\n",
    "    stmt = []\n",
    "    for h in d:\n",
    "        stmt.append(vocab_base.index(h.lower()))\n",
    "    embeddings.append(stmt)\n",
    "    if len(d) < max_length:\n",
    "        padding = max_length - len(d)\n",
    "        stmt.extend(list(np.zeros(padding,dtype=np.int8)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 1092)\n"
     ]
    }
   ],
   "source": [
    "# Turn embeddings into np array\n",
    "X = np.array([np.array(xi) for xi in embeddings])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 171\n",
      "INFO:lda:vocab_size: 1092\n",
      "INFO:lda:n_words: 75237140\n",
      "INFO:lda:n_topics: 5\n",
      "INFO:lda:n_iter: 700\n",
      "INFO:lda:<0> log likelihood: -625306577\n",
      "INFO:lda:<10> log likelihood: -566377507\n",
      "INFO:lda:<20> log likelihood: -526344831\n",
      "INFO:lda:<30> log likelihood: -521930268\n",
      "INFO:lda:<40> log likelihood: -521096066\n",
      "INFO:lda:<50> log likelihood: -521294667\n",
      "INFO:lda:<60> log likelihood: -521426356\n",
      "INFO:lda:<70> log likelihood: -521490812\n",
      "INFO:lda:<80> log likelihood: -521353186\n",
      "INFO:lda:<90> log likelihood: -521223351\n",
      "INFO:lda:<100> log likelihood: -521131788\n",
      "INFO:lda:<110> log likelihood: -521017657\n",
      "INFO:lda:<120> log likelihood: -520857315\n",
      "INFO:lda:<130> log likelihood: -520670322\n",
      "INFO:lda:<140> log likelihood: -520508173\n",
      "INFO:lda:<150> log likelihood: -520351314\n",
      "INFO:lda:<160> log likelihood: -520207803\n",
      "INFO:lda:<170> log likelihood: -520048442\n",
      "INFO:lda:<180> log likelihood: -519911142\n",
      "INFO:lda:<190> log likelihood: -519818859\n",
      "INFO:lda:<200> log likelihood: -519698197\n",
      "INFO:lda:<210> log likelihood: -519633756\n",
      "INFO:lda:<220> log likelihood: -519544448\n",
      "INFO:lda:<230> log likelihood: -519457261\n",
      "INFO:lda:<240> log likelihood: -519373741\n",
      "INFO:lda:<250> log likelihood: -519358442\n",
      "INFO:lda:<260> log likelihood: -519331481\n",
      "INFO:lda:<270> log likelihood: -519277761\n",
      "INFO:lda:<280> log likelihood: -519232578\n",
      "INFO:lda:<290> log likelihood: -519197719\n",
      "INFO:lda:<300> log likelihood: -519173195\n",
      "INFO:lda:<310> log likelihood: -519165947\n",
      "INFO:lda:<320> log likelihood: -519112805\n",
      "INFO:lda:<330> log likelihood: -519074565\n",
      "INFO:lda:<340> log likelihood: -519059597\n",
      "INFO:lda:<350> log likelihood: -519037899\n",
      "INFO:lda:<360> log likelihood: -519016419\n",
      "INFO:lda:<370> log likelihood: -519025602\n",
      "INFO:lda:<380> log likelihood: -519042232\n",
      "INFO:lda:<390> log likelihood: -519023929\n",
      "INFO:lda:<400> log likelihood: -518997180\n",
      "INFO:lda:<410> log likelihood: -518993796\n",
      "INFO:lda:<420> log likelihood: -518983000\n",
      "INFO:lda:<430> log likelihood: -518944551\n",
      "INFO:lda:<440> log likelihood: -518945595\n",
      "INFO:lda:<450> log likelihood: -518931517\n",
      "INFO:lda:<460> log likelihood: -518914218\n",
      "INFO:lda:<470> log likelihood: -518921428\n",
      "INFO:lda:<480> log likelihood: -518886411\n",
      "INFO:lda:<490> log likelihood: -518880563\n",
      "INFO:lda:<500> log likelihood: -518872074\n",
      "INFO:lda:<510> log likelihood: -518879826\n",
      "INFO:lda:<520> log likelihood: -518869561\n",
      "INFO:lda:<530> log likelihood: -518904632\n",
      "INFO:lda:<540> log likelihood: -518918211\n",
      "INFO:lda:<550> log likelihood: -518924857\n",
      "INFO:lda:<560> log likelihood: -518918763\n",
      "INFO:lda:<570> log likelihood: -518918656\n",
      "INFO:lda:<580> log likelihood: -518909907\n",
      "INFO:lda:<590> log likelihood: -518875585\n",
      "INFO:lda:<600> log likelihood: -518851954\n",
      "INFO:lda:<610> log likelihood: -518856911\n",
      "INFO:lda:<620> log likelihood: -518843096\n",
      "INFO:lda:<630> log likelihood: -518846012\n",
      "INFO:lda:<640> log likelihood: -518825735\n",
      "INFO:lda:<650> log likelihood: -518815666\n",
      "INFO:lda:<660> log likelihood: -518832620\n",
      "INFO:lda:<670> log likelihood: -518852589\n",
      "INFO:lda:<680> log likelihood: -518847950\n",
      "INFO:lda:<690> log likelihood: -518873467\n",
      "INFO:lda:<699> log likelihood: -518854066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: trillion unusually normalization signify 1 having suggested 18\n",
      "Topic 1: timing advances facilitating 3/4 holiday indicators over end\n",
      "Topic 2: still-robust nearly top total data mcdonough dudley confirms\n",
      "Topic 3: pay obtain strengthened risen so weakness choice growth\n",
      "Topic 4: tools 25-basis reaffirmed and shortly observed in preferred\n"
     ]
    }
   ],
   "source": [
    "# Run LDA model\n",
    "model = lda.LDA(n_topics=5, n_iter=700, random_state=1)\n",
    "model.fit(X)  # model.fit_transform(X) is also available\n",
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame(topic_words)\n",
    "t.to_csv(\"topic_words1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. (a) Remove numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test string with embedded punctuation and numbers\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    " \n",
    "def clearup(s, chars):\n",
    "    return re.sub('[%s]' % chars, '', s).lower()\n",
    " \n",
    "s = 'This is %a t1e22st !st4ring6 w.it6h 87embed766ded punct,:ua-tion and nu=mbe]rS6.'\n",
    " \n",
    "print(clearup(s, string.punctuation+string.digits))\n",
    "\n",
    "statements_nonums = []\n",
    "for j in statements_tokenized:\n",
    "    state = []\n",
    "    for i in j:\n",
    "        i = clearup(i, string.punctuation+string.digits)\n",
    "        if i:\n",
    "            state.append(i)\n",
    "    statements_nonums.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1771"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new vocabulary\n",
    "words = []\n",
    "for i in statements_nonums:\n",
    "    for j in i:\n",
    "        words.append(j.lower())\n",
    "vocab_nonums = list(set(words))\n",
    "\n",
    "len(vocab_nonums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 words removed from vocab\n"
     ]
    }
   ],
   "source": [
    "# How many words were lost?\n",
    "print(str(len(vocab_base) - len(vocab_nonums)) + ' words removed from vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "967"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine longest statements\n",
    "max_length = max([len(i) for i in statements_nonums])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 967)\n"
     ]
    }
   ],
   "source": [
    "# Get tokens by finding vocab indices\n",
    "embeddings = []\n",
    "for d in statements_nonums:\n",
    "    stmt = []\n",
    "    for h in d:\n",
    "        stmt.append(vocab_nonums.index(h.lower()))\n",
    "    embeddings.append(stmt)\n",
    "    if len(d) < max_length:\n",
    "        padding = max_length - len(d)\n",
    "        stmt.extend(list(np.zeros(padding,dtype=np.int8)))\n",
    "        \n",
    "# Turn embeddings into np array\n",
    "X = np.array([np.array(xi) for xi in embeddings])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 171\n",
      "INFO:lda:vocab_size: 967\n",
      "INFO:lda:n_words: 64027649\n",
      "INFO:lda:n_topics: 5\n",
      "INFO:lda:n_iter: 200\n",
      "INFO:lda:<0> log likelihood: -526134615\n",
      "INFO:lda:<10> log likelihood: -485313855\n",
      "INFO:lda:<20> log likelihood: -442209375\n",
      "INFO:lda:<30> log likelihood: -437375939\n",
      "INFO:lda:<40> log likelihood: -437512488\n",
      "INFO:lda:<50> log likelihood: -437686952\n",
      "INFO:lda:<60> log likelihood: -437766753\n",
      "INFO:lda:<70> log likelihood: -437769266\n",
      "INFO:lda:<80> log likelihood: -437742172\n",
      "INFO:lda:<90> log likelihood: -437690209\n",
      "INFO:lda:<100> log likelihood: -437623648\n",
      "INFO:lda:<110> log likelihood: -437565424\n",
      "INFO:lda:<120> log likelihood: -437494498\n",
      "INFO:lda:<130> log likelihood: -437436903\n",
      "INFO:lda:<140> log likelihood: -437380204\n",
      "INFO:lda:<150> log likelihood: -437318508\n",
      "INFO:lda:<160> log likelihood: -437266535\n",
      "INFO:lda:<170> log likelihood: -437205502\n",
      "INFO:lda:<180> log likelihood: -437156619\n",
      "INFO:lda:<190> log likelihood: -437099943\n",
      "INFO:lda:<199> log likelihood: -437056814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: dislocation necessary fail experience uncertain convincingly jerome nonetheless\n",
      "Topic 1: timing persistent toll attributable reserve likelihood power kashkari\n",
      "Topic 2: damping evans no fostered he stable quarter damped\n",
      "Topic 3: shortfalls seizingup third check carry theprices conducive closer\n",
      "Topic 4: judgment ensuring longerrun constrained taking must steps interestsensitive\n"
     ]
    }
   ],
   "source": [
    "# Run LDA model\n",
    "model2 = lda.LDA(n_topics=5, n_iter=200, random_state=1)\n",
    "model2.fit(X)  # model.fit_transform(X) is also available\n",
    "topic_word = model2.topic_word_  # model.components_ also works\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab_nonums)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = statements['text']\n",
    "y = statements['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62857142857142856"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-and-machine-learning-on-documents-1.html\n",
    "https://gate.ac.uk/sale/nle-svm/svm-ie.pdf\n",
    "https://www.quora.com/How-do-I-train-a-SVM-classifier-from-text-examples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
